{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from eqtmZscore to withExpressionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bonder-eQTMsFDR0.0-CpGLevel-split.txt', '2017-12-09-eQTLsFDR-et0.0-flipped.txtsplit.txt', '2017-12-09-eQTLsFDR-et0.0-flipped.txt', 'westra-eQTMsFDR0.0-CpGLevel-split.txt', '2017-12-09-eQTLsFDR-gt0.0-flipped.txtsplit.txt', 'zScoresBonderFinalUnique.csv', 'bonder-eQTMsFDR0.0-CpGLevel-split-uniqueCGs.txt', 'zScoresWestraFinalUnique.csv', 'westra-eQTMsFDR0.0-CpGLevel-split-uniqueCGs.txt', '2017-12-09-eQTLsFDR-gt0.0-flipped.txt']\n",
      "['fdr_gt0.2_sm0.3.txt', 'random20000-eQTLsFDR-gt0.05-flipped.txt', 'random20k_gt0.5.txt', 'random20k_gt0.05_sm0.1.txt', 'random20k_gt0.1_sm0.2.txt', 'random20k_gt0.3_sm0.4.txt', 'random20k_gt0.2_sm0.3.txt', '2017-12-09-eQTLsFDR-gt0.05-flipped.txt', 'fdr_gt0.5.txt', 'random20k_gt0.4_sm0.5.txt', 'fdr_gt0.4_sm0.5.txt', 'random20k_gt0.5_withZscore.txt', 'random20k_gt0.5_withTss.txt', 'fdr_gt0.3_sm0.4.txt', 'fdr_gt0.1_sm0.2.txt', 'selectAccordingFDR.sh', 'fdr_gt0.05_sm0.1.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_FOLDER = '/home/shuang/projects/boxy_eqtm/data'\n",
    "EQTM_DATADIR = os.path.join(DATA_FOLDER,'eqtmZscores')\n",
    "INPUT_FOLDER_1 = os.path.join(EQTM_DATADIR,'fdr_sm0.05')\n",
    "filenames1 = ['2017-12-09-eQTLsFDR-et0.0-flipped.txt','2017-12-09-eQTLsFDR-gt0.0-flipped.txt']\n",
    "INPUT_FOLDER_2 = os.path.join(EQTM_DATADIR,'fdr_gt0.05')\n",
    "filenames2 = ['random20k_gt0.5_withTss.txt']\n",
    "OUTPUT_FOLDER = os.path.join(EQTM_DATADIR,'withExpressionData')\n",
    "print(os.listdir(INPUT_FOLDER_1))\n",
    "print(os.listdir(INPUT_FOLDER_2))\n",
    "\n",
    "# expression data\n",
    "expression_filepath = os.path.join(DATA_FOLDER,'features','meanVar','expression-MeanAndVarianceRows.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19960, 4) \n",
      "                 ID     N      Mean           Var\n",
      "0  ENSG00000180008  2905 -3.547676  48060.577013\n",
      "1  ENSG00000167693  2905  6.642518  47299.021217\n",
      "2  ENSG00000064703  2905  5.160337  48509.749511\n",
      "3  ENSG00000189292  2905 -2.695654  48425.761353\n",
      "4  ENSG00000243772  2905 -2.186263  48663.139410\n"
     ]
    }
   ],
   "source": [
    "# read expression data\n",
    "expression_data = pd.read_csv(expression_filepath,sep='\\t')\n",
    "print(expression_data.shape,'\\n',expression_data.head())\n",
    "expression_dict = expression_data[['ID','Mean','Var']].set_index('ID').T.to_dict('list')\n",
    "# print(expression_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-09-eQTLsFDR-et0.0-flipped.txt\n",
      "Saved to : /home/shuang/projects/boxy_eqtm/data/eqtmZscores/withExpressionData/2017-12-09-eQTLsFDR-et0_withExpression.txt\n",
      "2017-12-09-eQTLsFDR-gt0.0-flipped.txt\n",
      "Saved to : /home/shuang/projects/boxy_eqtm/data/eqtmZscores/withExpressionData/2017-12-09-eQTLsFDR-gt0_withExpression.txt\n",
      "random20k_gt0.5_withTss.txt\n",
      "random20k_gt0.5_withTss\n",
      "Saved to : /home/shuang/projects/boxy_eqtm/data/eqtmZscores/withExpressionData/random20k_gt0.5_withTss_withExpression.txt\n"
     ]
    }
   ],
   "source": [
    "# do the same thing for files in filenames1 and filenames2\n",
    "for filename in filenames1:\n",
    "    print(filename)\n",
    "    eqtm_name = '.'.join(filename.split('.')[:-2])\n",
    "    input_filepath = os.path.join(INPUT_FOLDER_1,filename)\n",
    "    eqtm = pd.read_csv(input_filepath,sep='\\t',index_col=0)\n",
    "    eqtm['expressionMean'] = [expression_dict[key][0] for key in eqtm['ProbeName'].values]\n",
    "    eqtm['expressionVar'] = [expression_dict[key][1] for key in eqtm['ProbeName'].values]\n",
    "    eqtm_savepath = os.path.join(OUTPUT_FOLDER,eqtm_name+'_withExpression.txt')\n",
    "    eqtm.to_csv(eqtm_savepath)\n",
    "    print('Saved to :',eqtm_savepath)\n",
    "\n",
    "for filename in filenames2:\n",
    "    print(filename)\n",
    "    eqtm_name = '.'.join(filename.split('.')[:-1])\n",
    "    print(eqtm_name)\n",
    "    input_filepath = os.path.join(INPUT_FOLDER_2,filename)\n",
    "    eqtm = pd.read_csv(input_filepath,sep=',',index_col=0)\n",
    "    eqtm['expressionMean'] = [expression_dict[key][0] for key in eqtm['ProbeName'].values]\n",
    "    eqtm['expressionVar'] = [expression_dict[key][1] for key in eqtm['ProbeName'].values]\n",
    "    eqtm_savepath = os.path.join(OUTPUT_FOLDER,eqtm_name+'_withExpression.txt')\n",
    "    eqtm.to_csv(eqtm_savepath)\n",
    "    print('Saved to :',eqtm_savepath)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from withExpressionData to withExpressionTSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "withExpressionDir = os.path.join(EQTM_DATADIR,'withExpressionData')\n",
    "withExpressionTSSDir = os.path.join(EQTM_DATADIR,'withExpressionTSS')\n",
    "tss_path = '/home/shuang/projects/boxy_eqtm/data/features/TSSDistance/Homo_sapiens.GRCh37.71.gtf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tss(tss_path):\n",
    "    # read tss file from tss_path\n",
    "    colnames = ['chr','regionFunction','regionType','startSite',\n",
    "                'endSite','score','strand','sthunknown','geneInfo']\n",
    "    dtype = {'chr':object,'regionFunction':object,'regionType':object,\n",
    "             'startSite':int,'endSite':int,'score':object,\n",
    "             'strand':object,'sthunknown':object,'geneInfo':object}\n",
    "    tss_raw = pd.read_csv(tss_path,sep='\\t',header=None,names=colnames,dtype=dtype)\n",
    "    return tss_raw\n",
    "\n",
    "def addTssDistance2eQTMwithZscoreFile(eQTM_path,tss_raw,save_path = None):\n",
    "    '''\n",
    "    add TssDistance to eqtmZscore file\n",
    "    INPUT:\n",
    "        eQTM_path, string, path to eqtmZscore file\n",
    "        tss_path, string, path to TssSite file\n",
    "        save_path, string, path to save the new eqtmZscore_withTSSDistance file\n",
    "    OUTPUT:\n",
    "        eQTMs, pandas dataframe\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # reading the eQTMs\n",
    "    eQTMs = pd.read_csv(eQTM_path,sep=',')\n",
    "\n",
    "    # extract gene name from geneInfo for tss file\n",
    "    def findGeneName(item):\n",
    "        item = [thing for thing in list(filter(None,item.strip().split(\";\")))][0]\n",
    "        name = item.replace('\"','').replace(';','').strip().split(' ')[1]\n",
    "        return name\n",
    "    tss_raw['geneName'] = tss_raw['geneInfo'].apply(findGeneName)\n",
    "\n",
    "    # find the tss sites for each gene in the tss file\n",
    "    groupbyTss = tss_raw.groupby('geneName').agg({\n",
    "        'chr':lambda x: x.unique(),\n",
    "        'startSite':np.min,\n",
    "        'endSite':np.max,\n",
    "        'strand':lambda x: x.unique()\n",
    "    })\n",
    "    def findTssSite(series):\n",
    "        if series[3] == '-':\n",
    "            return series[2]\n",
    "        else:\n",
    "            return series[1]\n",
    "    groupbyTss['TssSite'] = groupbyTss.apply(findTssSite,axis=1)\n",
    "\n",
    "    # add tss sites and tss distance to the eqtm file\n",
    "    def mapSite(row):\n",
    "        return groupbyTss.loc[row]['TssSite']\n",
    "    def calculateDis(row):\n",
    "        return abs(row[0]-row[1])\n",
    "    def findChr(row):\n",
    "        return groupbyTss.loc[row]['chr']\n",
    "    def checkChr(row):\n",
    "        if str(row[0])==str(row[1]):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    eQTMs['TssSite'] = eQTMs['ProbeName'].apply(mapSite)\n",
    "    eQTMs['chr'] = eQTMs['ProbeName'].apply(findChr)\n",
    "    eQTMs['TssDistance'] = eQTMs[['SNPChrPos','TssSite']].apply(calculateDis,axis=1)\n",
    "    eQTMs['checkChr'] = eQTMs[['chr','SNPChr']].apply(checkChr,axis=1)\n",
    "    # check whether they are from the same chromosome\n",
    "    assert len(eQTMs['checkChr'].unique()) == 1\n",
    "\n",
    "    if save_path:\n",
    "        # save the eQTM file\n",
    "        eQTMs.to_csv(save_path,index=False)\n",
    "        print('Saved eQTM file to: ',save_path)\n",
    "\n",
    "    return eQTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS file loaded.\n",
      "Saved eQTM file to:  /home/shuang/projects/boxy_eqtm/data/eqtmZscores/withExpressionTSS/2017-12-09-eQTLsFDR-gt0_withExpressionTss.txt\n",
      "Saved eQTM file to:  /home/shuang/projects/boxy_eqtm/data/eqtmZscores/withExpressionTSS/random20k_gt0.5_withTss_withExpressionTss.txt\n",
      "Saved eQTM file to:  /home/shuang/projects/boxy_eqtm/data/eqtmZscores/withExpressionTSS/2017-12-09-eQTLsFDR-et0_withExpressionTss.txt\n"
     ]
    }
   ],
   "source": [
    "tss_raw = read_tss(tss_path)\n",
    "print('TSS file loaded.')\n",
    "for filename in os.listdir(withExpressionDir):\n",
    "    eqtm_name = '.'.join(filename.split('.')[:-1])\n",
    "    eQTM_path = os.path.join(withExpressionDir,filename)\n",
    "    save_path = os.path.join(withExpressionTSSDir,eqtm_name+'Tss.txt')\n",
    "    _ = addTssDistance2eQTMwithZscoreFile(eQTM_path,tss_raw,save_path = save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from withExpressionTss to withExpressionTssMethy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "withExpressionTssMethyDir = os.path.join(EQTM_DATADIR,'withExpressionTSSMethy')\n",
    "methy_filepath = '/home/shuang/projects/boxy_eqtm/data/features/meanVar/methylation-MeanAndVarianceRows.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_methy(methy_filepath):\n",
    "    methy = pd.read_csv(methy_filepath,sep='\\t')\n",
    "    methy_dict = methy[['ID','Mean','Var']].set_index('ID').T.to_dict('list')\n",
    "    return methy_dict\n",
    "def add_Methy(input_filepath,methy_dict,output_filepath):\n",
    "    inputfile = pd.read_csv(input_filepath,sep=',',index_col=0)\n",
    "    inputfile['SNPName_ProbeName'] = ['{}_{}'.format(row[0],row[1]) \n",
    "                                      for row in inputfile[['SNPName','ProbeName']].values]\n",
    "    inputfile['methyMean'] = [methy_dict[row][0] for row in inputfile['SNPName'].values]\n",
    "    inputfile['methyVar'] = [methy_dict[row][1] for row in inputfile['SNPName'].values]\n",
    "    inputfile.to_csv(output_filepath)\n",
    "    print('Saved file to:',output_filepath)\n",
    "    return inputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methylation file loaded.\n",
      "Saved file to: /home/shuang/projects/boxy_eqtm/data/eqtmZscores/withExpressionTSSMethy/2017-12-09-eQTLsFDR-et0_withExpressionTssMethy.txt\n",
      "Saved file to: /home/shuang/projects/boxy_eqtm/data/eqtmZscores/withExpressionTSSMethy/2017-12-09-eQTLsFDR-gt0_withExpressionTssMethy.txt\n",
      "Saved file to: /home/shuang/projects/boxy_eqtm/data/eqtmZscores/withExpressionTSSMethy/random20k_gt0.5_withTss_withExpressionTssMethy.txt\n"
     ]
    }
   ],
   "source": [
    "# methy = read_methy(methy_filepath)\n",
    "print(\"Methylation file loaded.\")\n",
    "for filename in os.listdir(withExpressionTSSDir):\n",
    "    filepath = os.path.join(withExpressionTSSDir,filename)\n",
    "    eqtm_name = '.'.join(filename.split('.')[:-1])\n",
    "    save_path = os.path.join(withExpressionTssMethyDir,eqtm_name+'Methy.txt')\n",
    "    _ = add_Methy(filepath,methy,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add overlapRatio to eqtm files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqtm_withTSSExpressionMethy_dir = os.path.join(EQTM_DATADIR,'withExpressionTSSMethy')\n",
    "eqtm_overlapRatio_dir = os.path.join(DATA_FOLDER,'dataReadyForModeling','overlapRatioTssMeanVar')\n",
    "eqtm_overlap_withTSSExpressionMethy = os.path.join(EQTM_DATADIR,'final')\n",
    "et_overlap_name = '2017-12-09-eQTLsFDR-et0.0-flipped_overlapRatio.txt'\n",
    "et_eqtm_name = '2017-12-09-eQTLsFDR-et0_withExpressionTssMethy.txt'\n",
    "gt_overlap_name = '2017-12-09-eQTLsFDR-gt0.0-flipped_overlapRatio.txt'\n",
    "gt_eqtm_name = '2017-12-09-eQTLsFDR-gt0_withExpressionTssMethy.txt'\n",
    "random_overlap_name = 'random20k_gt0.5_overlapRatio.txt'\n",
    "random_eqtm_name = 'random20k_gt0.5_withTss_withExpressionTssMethy.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_overlap_to_eqtm(eqtm_filename, overlap_filename):\n",
    "    overlap_filepath = os.path.join(eqtm_overlapRatio_dir,overlap_filename)\n",
    "    overlap = pd.read_csv(overlap_filepath,sep='\\t',index_col=0)\n",
    "    \n",
    "    eqtm_filepath = os.path.join(eqtm_withTSSExpressionMethy_dir,eqtm_filename)\n",
    "    eqtm = pd.read_csv(eqtm_filepath,index_col=0)\n",
    "#     print(eqtm.head())\n",
    "#     print(overlap.head())\n",
    "    overlap_dict = overlap.T.to_dict()\n",
    "    for col in overlap.columns:\n",
    "        eqtm[col] = [overlap_dict[row][col] for row in eqtm['SNPName'].values]\n",
    "    eqtm.to_csv(os.path.join(eqtm_overlap_withTSSExpressionMethy,\n",
    "                             '.'.join(eqtm_filename.split('.')[:-1])+'Overlap.txt'))\n",
    "    return eqtm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = add_overlap_to_eqtm(et_eqtm_name,et_overlap_name)\n",
    "_ = add_overlap_to_eqtm(gt_eqtm_name,gt_overlap_name)\n",
    "_ = add_overlap_to_eqtm(random_eqtm_name,random_overlap_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concate all files together and save in final folder as multi_class.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_savepath = os.path.join(eqtm_overlap_withTSSExpressionMethy,'multi_class.txt')\n",
    "# eqtm_overlap_withTSSExpressionMethy\n",
    "multi_class = []\n",
    "for filename in os.listdir(eqtm_overlap_withTSSExpressionMethy)[1:]:\n",
    "    filepath = os.path.join(eqtm_overlap_withTSSExpressionMethy,filename)\n",
    "    filecontent = pd.read_csv(filepath)\n",
    "    multi_class.append(filecontent)\n",
    "multi_class_pd = pd.concat(multi_class)\n",
    "multi_class_pd.head()\n",
    "multi_class_pd.to_csv(multi_class_savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from lib.read.read_data import load_data\n",
    "from lib.read.dataset_class import dataset\n",
    "from lib.model.modelBuild import convModelObject\n",
    "from math import copysign\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def examineModel(model_name,trainPath,testPath,label_name='OverallZScore',savefilename=None,\n",
    "                 keep=[],exclude=[],display=True):\n",
    "    res = {'sensitivity':[],'specificity':[],'auc':[]}\n",
    "    res2 = {'sensitivity':[],'specificity':[],'auc':[]}\n",
    "    for i in range(4):\n",
    "        data = load_data(trainPath,label_name=label_name,keep=keep,exclude=exclude)\n",
    "        features_to_use = [col for col in data.train.values.columns if col not in keep]\n",
    "        train_data = dataset(data.train.values[features_to_use],data.train.labels.astype('int8'))\n",
    "        valid_data = dataset(data.test.values[features_to_use],data.test.labels.astype('int8'))\n",
    "\n",
    "        model = choose_model(model_name,len(features_to_use))\n",
    "        print(model.pipeline, model.param_grid)\n",
    "        model.savefilename = savefilename\n",
    "\n",
    "        sensitivity,specificity,auc = \\\n",
    "        model.train_and_display_output(train_data=train_data,\n",
    "                                 valid_data=valid_data,\n",
    "                                 returnRes=True,\n",
    "                                 display=display)\n",
    "        res['sensitivity'].append(sensitivity)\n",
    "        res['specificity'].append(specificity)\n",
    "        res['auc'].append(auc)\n",
    "        if trainPath != testPath:\n",
    "            test_data = load_data(testPath,keep=keep,\n",
    "                                  exclude=exclude,test_size=1).test\n",
    "            sensitivity2,specificity2,auc2 = \\\n",
    "            model.calculate_output(model.classifier,test_data)\n",
    "            print('>>> test on another dataset: sensitivity: '+\n",
    "            '{:.{prec}}\\tspecificity: {:.{prec}f}\\tauc:{}'.\\\n",
    "                  format(sensitivity2,specificity2,auc2,prec=3))\n",
    "            res2['sensitivity'].append(sensitivity2)\n",
    "            res2['specificity'].append(specificity2)\n",
    "            res2['auc'].append(auc2)\n",
    "    print('Sensitivity:',\n",
    "    np.mean(np.array(res['sensitivity'])),\n",
    "    np.std(np.array(res['sensitivity'])),\n",
    "    np.mean(np.array(res2['sensitivity'])),\n",
    "    np.std(np.array(res2['sensitivity'])))\n",
    "    print('Specificity:',\n",
    "    np.mean(np.array(res['specificity'])),\n",
    "    np.std(np.array(res['specificity'])),\n",
    "    np.mean(np.array(res2['specificity'])),\n",
    "    np.std(np.array(res2['specificity'])))\n",
    "    print('AUC:',\n",
    "    np.mean(np.array(res['auc'])),\n",
    "    np.std(np.array(res['auc'])),\n",
    "    np.mean(np.array(res2['auc'])),\n",
    "    np.std(np.array(res2['auc'])))\n",
    "\n",
    "def compare_Tss_meanVar_exclude(filepath,testpath):\n",
    "    exclude1 = ['cpgName','cpgName_split','TSS_Distance','methyMean','methyVar']\n",
    "    exclude2 = ['cpgName','cpgName_split','methyMean','methyVar']\n",
    "    exclude3 = ['cpgName','cpgName_split','TSS_Distance','methyVar']\n",
    "    exclude4 = ['cpgName','cpgName_split','TSS_Distance','methyMean']\n",
    "    exclude5 = ['cpgName','cpgName_split','methyMean']\n",
    "    exclude6 = ['cpgName','cpgName_split','methyVar']\n",
    "    exclude7 = ['cpgName','cpgName_split','TSS_Distance']\n",
    "    exclude8 = ['cpgName','cpgName_split']\n",
    "\n",
    "    # only check dataset with TSS and dataset with TSS & meanVariance\n",
    "    for exclude in [exclude2,exclude8]:\n",
    "        print('Not using features:',exclude)\n",
    "        modelPerformance(filepath,testpath,exclude=exclude)\n",
    "\n",
    "def choose_model(model_name,num_features):\n",
    "    if model_name=='ranfor':\n",
    "        pipeline = Pipeline(steps=[('pca',PCA()),('ranfor',\n",
    "                                           RandomForestClassifier())])\n",
    "        n_estimators = [10,50,100]\n",
    "        class_weight = ['balanced',{1:4,0:1},{1:2,0:1}]\n",
    "        n_components = np.arange(2,num_features,int(num_features/5))\n",
    "        param_grid = [{'pca__n_components':n_components,\n",
    "                       'ranfor__n_estimators':n_estimators,\n",
    "                       'ranfor__class_weight':class_weight}]\n",
    "    elif model_name=='knn':\n",
    "        pipeline = Pipeline(steps=[('kneighbor',\n",
    "                                    KNeighborsClassifier())])\n",
    "        # train the model!\n",
    "        n_neighbors = range(2,10,2)\n",
    "        weights = ['uniform','distance']\n",
    "        param_grid = [{'kneighbor__n_neighbors':n_neighbors,\n",
    "                       'kneighbor__weights':weights}]\n",
    "    model = convModelObject(name='model_name',\n",
    "                      pipeline=pipeline,\n",
    "                      param_grid=param_grid)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test binary classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded with shape: (7497, 62)\n",
      "Data Normalized.\n",
      "Check the null values:\n",
      "Here are columns with NaN values:\n",
      "Check the loaded dataset: \n",
      " train with shape (5622, 39)\n",
      "test with shape: (1875, 39)\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('ranfor', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "      ...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))]) [{'pca__n_components': array([ 2,  9, 16, 23, 30, 37]), 'ranfor__n_estimators': [10, 50, 100], 'ranfor__class_weight': ['balanced', {1: 4, 0: 1}, {1: 2, 0: 1}]}]\n",
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "PROJECT_DIR = '/home/shuang/projects/boxy_eqtm'\n",
    "data_folder = os.path.join(PROJECT_DIR,'data',\n",
    "                           'eqtmZscores','final')\n",
    "et_filepath = os.path.join(data_folder,'2017-12-09-eQTLsFDR-et0_withExpressionTssMethyOverlap.txt')\n",
    "gt_filepath = os.path.join(data_folder,'2017-12-09-eQTLsFDR-gt0_withExpressionTssMethyOverlap.txt')\n",
    "random_filepath = os.path.join(data_folder,'random20k_gt0.5_withTss_withExpressionTssMethyOverlap.txt')\n",
    "\n",
    "model_name = 'ranfor'\n",
    "trainPath = et_filepath\n",
    "testPath = et_filepath\n",
    "exclude = ['SNPName','SNPChr','PValue','SNPChrPos','ProbeName','ProbeChr','ProbeCenterChrPos', \n",
    "           'CisTrans', 'SNPType', 'AlleleAssessed','DatasetsWhereSNPProbePairIsAvailableAndPassesQC',\n",
    "           'DatasetsZScores', 'DatasetsNrSamples','IncludedDatasetsMeanProbeExpression',\n",
    "           'IncludedDatasetsProbeExpressionVariance', 'HGNCName',\n",
    "           'IncludedDatasetsCorrelationCoefficient', 'Meta-Beta (SE)', 'Beta (SE)',\n",
    "           'FoldChange', 'FDR','checkChr','SNPName_ProbeName']\n",
    "keep = ['OverallZScore']\n",
    "\n",
    "for filepath in [et_filepath,gt_filepath,random_filepath]:\n",
    "    examineModel(model_name,filepath,filepath,keep=keep,exclude=exclude,display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_filepath = os.path.join(eqtm_overlap_withTSSExpressionMethy,'multi_class.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
